\section{Evaluation}
% 	Around 2 - 3 pages
%     - Reference implementation
%      * Base reference
%     - Testing, and how it was done
%     - Different stages of implementations
%     - Compare
% -----
An evaluation can me made using the data gotten using the test scripts described in the implementation section \ref{section-Implementation}. All measurements is measured by counting the cycles it takes to run the code.

\subsection{Reference implementation}
The reference implementation uses Montgomery ladder approach for point multiplication, schoolbook multiplication, and branch less fixed time approaches to the arithmetic in general. This makes it easier to measure the cycles as it they wont change depending on its input. The reference implementation takes $79.794$ megacycles to run.

\subsection{Karatsuba}
The testing method described in \ref{karatTesting} has been used to find the best operand length for the Karatsuba algorithm. The results of the test scripts has been plotted into a chart, and can be seen in figure \ref{karatsubafigure}. Based on those results the best operand length must be $32$, as this results in the least cycles. \\ 
\begin{figure}[H]
\begin{subfigure}{\textwidth}
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{report/images/karatsuba.png}}
    \caption{Karatsuba operand length - cycles}
\label{karatsubafigure}
\end{subfigure}
\end{figure}
Using this operand length results in a measurement of $79.118$ megacycles. To compare the Karatsuba cycles with the reference implementation both have been plotted into a chart in figure \ref{karatsubacomparison}. This chart shows the difference between the two implementation is not magnificent, as the difference is $0.676$ megacycles.\\

\begin{figure}[H]
\begin{subfigure}{\textwidth}
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{report/images/karatsuba-compared.png}}
    \caption{Karatsuba operand lengths (black) compared to reference implementation (grey)}
\end{subfigure}
\label{karatsubacomparison}
\end{figure}


\subsection{Testing}
Testing was done using the setup described in subsection \ref{sub-testing}. This enables testing of different seeds, Karatsuba operand lengths and multiple iterations of the algorithm.
\\
The current setup only compares the reference implementation with Karatsuba on the emulated CPU. Other kind of tests should have been created as well, to cover more of the range of mistakes that could have happened.\\
\subsubsection{Test with multiple different seeds}
\label{missing-test-1}
A script to automatically generate a bunch of input files, and output files should have been made. Thee files would contain the seed in the input, and $n$ checksums in the output files. Which the test-script then could load in and automatically verify that all the input matched with the output files. This would help verify that the reference and modified implementations matched over multiple different seeds.

\subsubsection{Comparing to the local reference implementation}
The same input and output files could have been made to compare it with the local reference implementation located in \texttt{src/ref/}. In this case the input files would contain \textit{p} and \textit{m} used in \textit{crypto\_scalarmult} and the output files would contain the checksum.\\ The test script could then load in theese files just as described in subsection \ref{missing-test-1} This would help verify that the ported version matches the reference implementation.

\subsubsection{CLI flags}
Different CLI flags could have been made for the test-script. This would have allowed to switch between different tests easily, and without having to modify the code itself.
\pagebreak